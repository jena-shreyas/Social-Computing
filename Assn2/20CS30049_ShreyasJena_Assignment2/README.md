# Instructions :

- To run the notebooks on Google Colab, it is recommended to select 'Tesla T4' GPU from the 'Change runtime type' option, as the BERT-based transformer models train faster on GPUs.
- Training in some cases might take around 10 mins, depending on model complexity, batch size and dataset size.
- For the direct and bridge translation tasks, the translation time can depend on the Google Translate API latency, which might even go upto 20 minutes for the bridge task on WebMD dataset.
- Run each cell one by one, and get model performance metrics at the end.

