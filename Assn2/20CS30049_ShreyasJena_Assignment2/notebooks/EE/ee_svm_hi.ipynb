{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "-mW9sMUV4Drz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWzVUueb0Ml-",
        "outputId": "eac1330f-933c-48a1-f63b-c83df4bc45dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indic-health-demo'...\n",
            "remote: Enumerating objects: 231, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 231 (delta 55), reused 103 (delta 43), pack-reused 110\u001b[K\n",
            "Receiving objects: 100% (231/231), 1.24 MiB | 4.62 MiB/s, done.\n",
            "Resolving deltas: 100% (99/99), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/indichealth/indic-health-demo.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd indic-health-demo/Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK5ky-rj2e4x",
        "outputId": "099afe03-25e5-4753-9cda-af1388e701a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/indic-health-demo/Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "xp1FOUSf4xLd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'IHQID-WebMD'"
      ],
      "metadata": {
        "id": "d2oab_sR24Sd"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
        "test = pd.read_csv(os.path.join(data_path, 'test.csv'))"
      ],
      "metadata": {
        "id": "7doSsTyq402f"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def biotagging(data: pd.DataFrame, lang):\n",
        "  data = data.fillna('') # replace nan values with empty string\n",
        "  # add column for tokens from tokenizer\n",
        "  def tokens(text):\n",
        "    return [token.lower() for token in text.split()]\n",
        "\n",
        "  cols = [f'question_{lang}',\n",
        "          f'disease_{lang}',\n",
        "          f'drug_{lang}',\n",
        "          f'treatment_{lang}',]\n",
        "  for col in cols:\n",
        "    data[col + '_tokens'] = data[col].apply(tokens)\n",
        "\n",
        "  def biotag(row):\n",
        "    qn_tokens = row[f'question_{lang}_tokens']\n",
        "    dis_tokens = row[f'disease_{lang}_tokens']\n",
        "    drug_tokens = row[f'drug_{lang}_tokens']\n",
        "    treat_tokens = row[f'treatment_{lang}_tokens']\n",
        "\n",
        "    i = 0\n",
        "    biotags = []\n",
        "    while i < len(qn_tokens):\n",
        "      token = qn_tokens[i]\n",
        "      # if current token matches with the starting tokens of annotated disease, drug or treatment\n",
        "      if ((len(dis_tokens) > 0 and (token in dis_tokens[0] or dis_tokens[0] in token)) or\n",
        "         (len(drug_tokens) > 0 and (token in drug_tokens[0] or drug_tokens[0] in token)) or\n",
        "         (len(treat_tokens) > 0 and (token in treat_tokens[0] or treat_tokens[0] in token))):\n",
        "\n",
        "        entity = \"\"   # label for detected entity\n",
        "        if len(dis_tokens) > 0 and (token in dis_tokens[0] or dis_tokens[0] in token):\n",
        "          entity_tokens = dis_tokens\n",
        "          entity = \"disease\"\n",
        "        elif len(drug_tokens) > 0 and (token in drug_tokens[0] or drug_tokens[0] in token):\n",
        "          entity_tokens = drug_tokens\n",
        "          entity = \"drug\"\n",
        "        elif len(treat_tokens) > 0 and (token in treat_tokens[0] or treat_tokens[0] in token):\n",
        "          entity_tokens = treat_tokens\n",
        "          entity = \"treatment\"\n",
        "\n",
        "        # define matching function to compute similarity of entity and question subpart's tokens\n",
        "        def match_tokens(ent_toks, qn_toks, thresh):\n",
        "          cnt = 0\n",
        "          match_toks = []   # store the set of question tokens which match with entity tokens\n",
        "          for ent_tok in ent_toks:\n",
        "            for qn_tok in qn_toks:\n",
        "              if (ent_tok in qn_tok) or (qn_tok in ent_tok):\n",
        "                cnt += 1\n",
        "                match_toks.append(qn_tok)\n",
        "                break\n",
        "\n",
        "          match_toks = list(set(match_toks))\n",
        "          return cnt/len(ent_toks), match_toks\n",
        "\n",
        "        # if detected entity approximately matches with current substring based on threshold value\n",
        "        thresh = 0.4\n",
        "        # print(entity_tokens)\n",
        "        # print(qn_tokens[i:min(i+len(entity_tokens), len(qn_tokens))])\n",
        "        f_match, match_toks = match_tokens(entity_tokens, qn_tokens[i:min(i+len(entity_tokens), len(qn_tokens))], thresh)\n",
        "\n",
        "        # print(\"Match \", f_match)\n",
        "        # print(\"Match toks \", match_toks)\n",
        "        if f_match >= thresh:\n",
        "          idx = i   # store current value of idx\n",
        "          # add B, I tags for all tokens matching with those of the detected entity\n",
        "          while i < min(idx + len(entity_tokens), len(qn_tokens)):\n",
        "            if qn_tokens[i] in match_toks:\n",
        "              if i == idx:\n",
        "                biotags.append(f'B-{entity}')\n",
        "              else:\n",
        "                biotags.append(f'I-{entity}')\n",
        "            else:\n",
        "              biotags.append('O')\n",
        "            i += 1\n",
        "          continue  # to prevent skipping an extra index\n",
        "        else: # since entity thought to match doesn't actually match completely, it is not the start token of some entity. So, add O biotag to the current token only\n",
        "          biotags.append('O')\n",
        "\n",
        "      else:\n",
        "        biotags.append('O')\n",
        "      i+=1\n",
        "\n",
        "    # The above approach might cause some intermediate tokens for any entity to be labelled O, even though its surrounding tokens are B/I tokens. We fix such tokens as postprocessing\n",
        "    i = 0\n",
        "    while i < len(biotags):\n",
        "      biotag = biotags[i]\n",
        "      if (i>0 and i<len(biotags)-1):\n",
        "        prev_tag = biotags[i-1]\n",
        "        next_tag = biotags[i+1]\n",
        "        if (('B-' in prev_tag or 'I-' in prev_tag) and 'I-' in next_tag):\n",
        "          entity = prev_tag[2:]\n",
        "          biotags[i] = 'I-' + entity\n",
        "      i+=1\n",
        "\n",
        "    assert len(biotags) == len(qn_tokens)\n",
        "    return biotags\n",
        "\n",
        "  data[f'question_{lang}_biotags'] = data.apply(biotag, axis=1)\n",
        "  return data"
      ],
      "metadata": {
        "id": "st0LChMe_p5A"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = biotagging(train.iloc[[18]], 'english')\n",
        "print(\"Question : \", x.iloc[0][['question_english']].values[0])\n",
        "print(\"Disease : \", x.iloc[0][['disease_english']].values[0])\n",
        "print(\"Drug : \", x.iloc[0][['drug_english']].values[0])\n",
        "print(\"Treatment : \", x.iloc[0][['treatment_english']].values[0])\n",
        "print(x.iloc[0][['question_english_tokens']].values[0])\n",
        "print(x.iloc[0][['question_english_biotags']].values[0])"
      ],
      "metadata": {
        "id": "okC6Qz_BZ8_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2056a880-4819-45c3-db0a-1e92a3ec6663"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question :  scoliosis. does it effect the stomach and breathing.\n",
            "Disease :  Scoliosis\n",
            "Drug :  \n",
            "Treatment :  \n",
            "['scoliosis.', 'does', 'it', 'effect', 'the', 'stomach', 'and', 'breathing.']\n",
            "['B-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = biotagging(train, 'english')\n",
        "test = biotagging(test, 'english')"
      ],
      "metadata": {
        "id": "6HUXIVlXaxfd"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation"
      ],
      "metadata": {
        "id": "KHh7iMzKEpDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D8cjm9Z5RA-",
        "outputId": "eac0b458-96af-4775-80b6-0f3f6160cfcc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-11 15:11:38.673111: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-11 15:11:38.673183: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-11 15:11:38.673228: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-11 15:11:38.686638: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-11 15:11:41.996870: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "# Load the spacy model: nlp\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "metadata": {
        "id": "-efaCojN5fXG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the dimensionality of nlp\n",
        "embedding_dim = nlp.vocab.vectors_length\n",
        "print(embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PJsR2IL6YSu",
        "outputId": "7bed1580-fbfb-476b-e910-c9c16f85f360"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_tokens(tokenized_sentences):\n",
        "  tokens = []\n",
        "  for sent_tokens in list(tokenized_sentences):\n",
        "    tokens.extend(sent_tokens)\n",
        "  print(len(tokens))\n",
        "  X = np.zeros((len(tokens), embedding_dim))\n",
        "\n",
        "  for idx, token in enumerate(tokens):\n",
        "        doc = nlp(token)\n",
        "        X[idx, :] = doc.vector\n",
        "  return X\n",
        "\n",
        "train_token_emb = encode_tokens(train['question_english_tokens'])\n",
        "test_token_emb = encode_tokens(test['question_english_tokens'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skmf540JBPKZ",
        "outputId": "78a3acd6-8443-409c-8cd9-8b284ff37f38"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7127\n",
            "2513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "wAiUrRTnEt5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tag2label = {'O': 0,\n",
        " 'B-disease': 1,\n",
        " 'I-disease': 2,\n",
        " 'B-drug': 3,\n",
        " 'I-drug': 4,\n",
        " 'B-treatment': 5,\n",
        " 'I-treatment': 6}\n",
        "\n",
        "def flatten_labels(labels):\n",
        "  flat_labels = []\n",
        "  for sent_labels in list(labels):\n",
        "    flat_labels.extend(sent_labels)\n",
        "  print(len(flat_labels))\n",
        "\n",
        "  flat_labels = [tag2label[tag] for tag in flat_labels]\n",
        "  return flat_labels\n",
        "\n",
        "train_labels = flatten_labels(train['question_english_biotags'])\n",
        "test_labels = flatten_labels(test['question_english_biotags'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbg0w2-FE36U",
        "outputId": "53619b5a-b650-4806-e19b-98e01d7dccfb"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7127\n",
            "2513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(train_token_emb) == len(train_labels)\n",
        "assert len(test_token_emb) == len(test_labels)"
      ],
      "metadata": {
        "id": "NLdxMB3JFW6O"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "def train_svc(embeddings, labels):\n",
        "    clf = SVC(C = 1)\n",
        "    clf.fit(embeddings, labels)\n",
        "    return clf\n",
        "\n",
        "model = train_svc(train_token_emb, train_labels)"
      ],
      "metadata": {
        "id": "d5wgG_A2-LzO"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "EJWKE8bnEyto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def test_svc(model, embeddings, labels):\n",
        "\n",
        "    pred_labels = model.predict(embeddings)\n",
        "\n",
        "    # Count the number of correct predictions\n",
        "    correct = 0\n",
        "    for i in range(len(labels)):\n",
        "      if pred_labels[i] == labels[i]:\n",
        "        correct += 1\n",
        "\n",
        "    return pred_labels\n",
        "\n",
        "pred_labels = test_svc(model, test_token_emb, test_labels)\n",
        "print(classification_report(test_labels, pred_labels))\n",
        "print(confusion_matrix(test_labels, pred_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wdo381_-QsF",
        "outputId": "f5f0e1de-b76d-42f3-f355-f1ea23164898"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.99      0.91      2045\n",
            "           1       0.62      0.16      0.26       160\n",
            "           2       0.36      0.06      0.10        83\n",
            "           3       0.65      0.19      0.30        89\n",
            "           4       0.60      0.13      0.21        46\n",
            "           5       0.00      0.00      0.00        52\n",
            "           6       0.80      0.21      0.33        38\n",
            "\n",
            "    accuracy                           0.83      2513\n",
            "   macro avg       0.55      0.25      0.30      2513\n",
            "weighted avg       0.78      0.83      0.78      2513\n",
            "\n",
            "[[2019    8    8    5    3    0    2]\n",
            " [ 133   26    0    1    0    0    0]\n",
            " [  73    5    5    0    0    0    0]\n",
            " [  71    1    0   17    0    0    0]\n",
            " [  37    1    0    2    6    0    0]\n",
            " [  50    1    0    1    0    0    0]\n",
            " [  28    0    1    0    1    0    8]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vL01kn4RKCzG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}